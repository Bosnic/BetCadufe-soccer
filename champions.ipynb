{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxpLwCebj/46k6m0SQaMdF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bosnic/BetCadufe-soccer/blob/main/champions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJdixfnbLC1U"
      },
      "outputs": [],
      "source": [
        "# Importa as bibliotecas necessárias\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime, date, timedelta\n",
        "from google.cloud import bigquery\n",
        "from google.api_core.exceptions import NotFound"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Autenticação no Google Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ],
      "metadata": {
        "id": "NUGJs8aGMidq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define as variáveis de ambiente\n",
        "API_KEY = '95968922439b247c097e74f896df90b1' # Chave da API da API-Sports\n",
        "API_URL = 'https://v3.football.api-sports.io' # URL base da API da API-Sports\n",
        "PROJECT_ID = 'betcadufe-soccer' # ID do projeto no BigQuery\n",
        "DATASET_NAME = 'soccer' # Nome do dataset no BigQuery\n",
        "FULL_LOAD_DATE = '2022-01-01' # Data para carga completa inicial\n",
        "LEAGUE = 2 # ID da liga na API-Sports\n",
        "SEASON = 2022 # Ano da temporada\n",
        "\n",
        "# Define o cabeçalho da requisição para a API\n",
        "HEADERS = {\n",
        "    'x-rapidapi-key': API_KEY\n",
        "}"
      ],
      "metadata": {
        "id": "BHH76mXmMxIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define os endpoints da API a serem consumidos\n",
        "endpoints = [\n",
        "    {\n",
        "        \"table\": \"past_fixtures\", # Nome da tabela no BigQuery\n",
        "        \"write_disposition\": \"WRITE_APPEND\", # Define o método de escrita no BigQuery (append)\n",
        "        \"path\": \"fixtures\", # Caminho do endpoint na API-Sports\n",
        "        \"quality_control\": False, # Flag para controle de qualidade (não utilizado neste caso)\n",
        "        \"params\": { # Parâmetros da requisição para a API-Sports\n",
        "            \"league\": LEAGUE,\n",
        "            \"season\": SEASON\n",
        "        },\n",
        "        \"incremental_load_params\": { # Parâmetros para a carga incremental\n",
        "            \"from\": \"YYYY-MM-DD\", # Data de início para a carga incremental\n",
        "            \"to\": \"YYYY-MM-DD\" # Data de fim para a carga incremental\n",
        "        },\n",
        "        \"fields\": [], # Campos a serem selecionados na resposta da API (não utilizado neste caso)\n",
        "        \"nested_fields\": [ # Campos aninhados a serem extraídos da resposta da API\n",
        "            \"fixture.id\",\n",
        "            \"fixture.referee\",\n",
        "            \"fixture.timezone\",\n",
        "            \"fixture.date\",\n",
        "            \"fixture.timestamp\",\n",
        "            \"fixture.periods.first\",\n",
        "            \"fixture.periods.second\",\n",
        "            \"fixture.venue.id\",\n",
        "            \"fixture.venue.name\",\n",
        "            \"fixture.venue.city\",\n",
        "            \"fixture.status.long\",\n",
        "            \"fixture.status.short\",\n",
        "            \"fixture.status.elapsed\",\n",
        "            \"league.id\",\n",
        "            \"league.name\",\n",
        "            \"league.country\",\n",
        "            \"league.logo\",\n",
        "            \"league.flag\",\n",
        "            \"league.season\",\n",
        "            \"league.round\",\n",
        "            \"teams.home.id\",\n",
        "            \"teams.home.name\",\n",
        "            \"teams.home.logo\",\n",
        "            \"teams.home.winner\",\n",
        "            \"teams.away.id\",\n",
        "            \"teams.away.name\",\n",
        "            \"teams.away.logo\",\n",
        "            \"teams.away.winner\",\n",
        "            \"goals.home\",\n",
        "            \"goals.away\",\n",
        "            \"score.halftime.home\",\n",
        "            \"score.halftime.away\",\n",
        "            \"score.fulltime.home\",\n",
        "            \"score.fulltime.away\",\n",
        "            \"score.extratime.home\",\n",
        "            \"score.extratime.away\",\n",
        "            \"score.penalty.home\",\n",
        "            \"score.penalty.away\"\n",
        "        ],\n",
        "        \"repeatable_fields\": [] # Campos que se repetem na resposta da API (não utilizado neste caso)\n",
        "    },\n",
        "    # Próximos endpoints seguem a mesma estrutura do anterior\n",
        "    {\n",
        "        \"table\": \"future_fixtures\",\n",
        "        \"write_disposition\": \"WRITE_TRUNCATE\", # Define o método de escrita no BigQuery (truncate)\n",
        "        \"path\": \"fixtures\",\n",
        "        \"quality_control\": False,\n",
        "        \"params\": {\n",
        "            \"league\": LEAGUE,\n",
        "            \"season\": SEASON,\n",
        "            \"to\": \"2099-12-31\"\n",
        "        },\n",
        "        \"incremental_load_params\": {\n",
        "            \"from\": \"YYYY-MM-DD\",\n",
        "        },\n",
        "        \"fields\": [],\n",
        "        \"nested_fields\": [\n",
        "            \"fixture.id\",\n",
        "            \"fixture.timezone\",\n",
        "            \"fixture.date\",\n",
        "            \"fixture.timestamp\",\n",
        "            \"fixture.venue.id\",\n",
        "            \"fixture.venue.name\",\n",
        "            \"fixture.venue.city\",\n",
        "            \"fixture.status.long\",\n",
        "            \"fixture.status.short\",\n",
        "            \"league.id\",\n",
        "            \"league.name\",\n",
        "            \"league.country\",\n",
        "            \"league.logo\",\n",
        "            \"league.flag\",\n",
        "            \"league.season\",\n",
        "            \"league.round\",\n",
        "            \"teams.home.id\",\n",
        "            \"teams.home.name\",\n",
        "            \"teams.home.logo\",\n",
        "            \"teams.away.id\",\n",
        "            \"teams.away.name\",\n",
        "            \"teams.away.logo\"\n",
        "        ],\n",
        "        \"repeatable_fields\": []\n",
        "    },\n",
        "    {\n",
        "        \"table\": \"players\",\n",
        "        \"write_disposition\": \"WRITE_TRUNCATE\", # Define o método de escrita no BigQuery (truncate)\n",
        "        \"path\": \"players\",\n",
        "        \"quality_control\": True, # Flag para controle de qualidade (não utilizado neste caso)\n",
        "        \"params\": {\n",
        "            \"league\": LEAGUE,\n",
        "            \"season\": SEASON,\n",
        "            \"page\": 1 # Número da página de resultados da API\n",
        "        },\n",
        "        \"fields\": [],\n",
        "        \"nested_fields\": [\n",
        "            \"player.id\",\n",
        "            \"player.name\",\n",
        "            \"player.firstname\",\n",
        "            \"player.lastname\",\n",
        "            \"player.age\",\n",
        "            \"player.birth.date\",\n",
        "            \"player.birth.place\",\n",
        "            \"player.nationality\",\n",
        "            \"player.height\",\n",
        "            \"player.weight\",\n",
        "            \"player.injured\",\n",
        "            \"player.photo\"\n",
        "        ],\n",
        "        \"repeatable_fields\": []\n",
        "    },\n",
        "]\n",
        "\n",
        "# Define os endpoints da API a serem consumidos de forma iterativa,\n",
        "# ou seja, para cada item do endpoint principal, este endpoint será chamado\n",
        "iterable_endpoints = {\n",
        "    \"past_fixtures\": [ # Define os endpoints iteráveis para o endpoint \"past_fixtures\"\n",
        "        {\n",
        "            \"table\": \"fixturesStatistics\", # Nome da tabela no BigQuery\n",
        "            \"write_disposition\": \"WRITE_APPEND\", # Define o método de escrita no BigQuery (append)\n",
        "            \"path\": \"fixtures/statistics\", # Caminho do endpoint na API-Sports\n",
        "            \"query_param\": { # Define o parâmetro da requisição para a API-Sports que será usado para iterar sobre os dados do endpoint principal\n",
        "                \"fixture\": \"fixture.id\" # O valor do campo \"fixture.id\" do endpoint principal será usado como valor para o parâmetro \"fixture\" da requisição\n",
        "            },\n",
        "            \"fixed_params\": {}, # Parâmetros fixos da requisição para a API-Sports\n",
        "            \"fields\": [\"fixture\"], # Campos a serem selecionados na resposta da API\n",
        "            \"nested_fields\": [ # Campos aninhados a serem extraídos da resposta da API\n",
        "                \"team.id\",\n",
        "                \"team.name\",\n",
        "                \"team.logo\"\n",
        "            ],\n",
        "            \"repeatable_fields\": [ # Campos que se repetem na resposta da API\n",
        "                \"statistics\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"table\": \"fixturesLineups\", # Nome da tabela no BigQuery\n",
        "            \"write_disposition\": \"WRITE_APPEND\", # Define o método de escrita no BigQuery (append)\n",
        "            \"path\": \"fixtures/lineups\", # Caminho do endpoint na API-Sports\n",
        "            \"query_param\": { # Define o parâmetro da requisição para a API-Sports que será usado para iterar sobre os dados do endpoint principal\n",
        "                \"fixture\": \"fixture.id\" # O valor do campo \"fixture.id\" do endpoint principal será usado como valor para o parâmetro \"fixture\" da requisição\n",
        "            },\n",
        "            \"fixed_params\": {}, # Parâmetros fixos da requisição para a API-Sports\n",
        "            \"fields\": [ # Campos a serem selecionados na resposta da API\n",
        "                \"fixture\",\n",
        "                \"formation\"\n",
        "            ],\n",
        "            \"nested_fields\": [ # Campos aninhados a serem extraídos da resposta da API\n",
        "                \"team.id\"\n",
        "            ],\n",
        "            \"repeatable_fields\": [ # Campos que se repetem na resposta da API\n",
        "                \"startXI\"\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "2kHe8L9QPz5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para buscar os dados da API\n",
        "def fetch_data(path, params, headers):\n",
        "    \"\"\"\n",
        "    Busca dados da API em várias páginas.\n",
        "\n",
        "    Args:\n",
        "        path (str): O caminho do endpoint da API.\n",
        "        params (dict): Os parâmetros da consulta da API.\n",
        "        headers (dict): Os cabeçalhos da requisição da API.\n",
        "\n",
        "    Returns:\n",
        "        list: Uma lista de dicionários contendo os dados da API.\n",
        "    \"\"\"\n",
        "    all_data = [] # Inicializa uma lista vazia para armazenar todos os dados da API\n",
        "    while True: # Loop infinito para buscar dados de todas as páginas\n",
        "        response = requests.get(f\"{API_URL}/{path}\", headers=headers, params=params) # Faz a requisição para a API\n",
        "        data = response.json() # Converte a resposta da API para um dicionário Python\n",
        "        print(response.text) # Imprime a resposta da API no console\n",
        "        if 'response' in data and data['response']: # Verifica se a resposta da API contém dados\n",
        "            all_data.extend(data['response']) # Adiciona os dados da página atual à lista de todos os dados\n",
        "            if 'page' in params: # Verifica se a requisição foi feita com paginação\n",
        "                params['page'] += 1 # Incrementa o número da página para a próxima requisição\n",
        "            else:\n",
        "                break # Sai do loop se não houver paginação\n",
        "        else:\n",
        "            break # Sai do loop se não houver dados na resposta da API\n",
        "    return all_data # Retorna a lista de todos os dados da API\n",
        "\n",
        "# Função para buscar dados de endpoints iteráveis\n",
        "def fetch_iterable_data(main_data, iterable_endpoint):\n",
        "    \"\"\"\n",
        "    Busca dados de um endpoint iterável para cada item em um DataFrame principal.\n",
        "\n",
        "    Args:\n",
        "        main_data (pd.DataFrame): O DataFrame principal contendo dados de um endpoint anterior.\n",
        "        iterable_endpoint (dict): As configurações do endpoint iterável.\n",
        "\n",
        "    Returns:\n",
        "        list: Uma lista de dicionários contendo os dados do endpoint iterável para cada item no DataFrame principal.\n",
        "    \"\"\"\n",
        "    all_data = [] # Inicializa uma lista para armazenar todos os dados\n",
        "    for _, item in main_data.iterrows(): # Itera sobre cada linha do DataFrame principal\n",
        "        query_params = {key: item[value.replace('.', '__')] for key, value in iterable_endpoint[\"query_param\"].items()} # Cria os parâmetros de consulta para o endpoint iterável com base nos valores da linha atual do DataFrame principal\n",
        "        params = query_params.copy() # Copia os parâmetros de consulta\n",
        "        params.update(iterable_endpoint['fixed_params']) # Atualiza os parâmetros de consulta com os parâmetros fixos do endpoint iterável\n",
        "        data = fetch_data(iterable_endpoint['path'], params, HEADERS) # Busca os dados do endpoint iterável\n",
        "\n",
        "        iterated_data = [{**query_params, **item} for item in data] # Combina os parâmetros de consulta com os dados da resposta\n",
        "        all_data.extend(iterated_data) # Adiciona os dados combinados à lista de todos os dados\n",
        "    return all_data # Retorna a lista de todos os dados\n",
        "\n",
        "# Função para preparar o DataFrame\n",
        "def prepare_dataframe(data, fields, nested_fields, repeatable_fields):\n",
        "    \"\"\"\n",
        "    Prepara um DataFrame a partir de uma lista de dicionários, extraindo campos aninhados e repetíveis.\n",
        "\n",
        "    Args:\n",
        "        data (list): A lista de dicionários contendo os dados.\n",
        "        fields (list): Uma lista de nomes de campos a serem incluídos no DataFrame.\n",
        "        nested_fields (list): Uma lista de nomes de campos aninhados a serem extraídos.\n",
        "        repeatable_fields (list): Uma lista de nomes de campos repetíveis a serem extraídos.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: O DataFrame preparado.\n",
        "    \"\"\"\n",
        "    for item in data: # Itera sobre cada dicionário na lista de dados\n",
        "        for field in repeatable_fields: # Itera sobre cada campo repetível\n",
        "            if field in item: # Verifica se o campo repetível existe no dicionário atual\n",
        "                for sub_item in item[field]: # Itera sobre cada item na lista do campo repetível\n",
        "                    for key in sub_item.keys(): # Itera sobre cada chave no item da lista do campo repetível\n",
        "                        sub_item[key] = str(sub_item[key]) # Converte o valor da chave para string\n",
        "\n",
        "    meta_fields = fields + repeatable_fields # Cria uma lista de campos que serão usados como metadados no DataFrame\n",
        "\n",
        "    df = pd.json_normalize(data, sep='__', meta=meta_fields) # Cria um DataFrame a partir da lista de dicionários, usando \"__\" como separador para campos aninhados\n",
        "\n",
        "    all_fields = fields + nested_fields + repeatable_fields # Cria uma lista de todos os campos desejados no DataFrame\n",
        "    column_names = [col.replace('.', '__') for col in all_fields] # Substitui \".\" por \"__\" nos nomes dos campos para corresponder ao formato do DataFrame\n",
        "\n",
        "    existing_columns = [col for col in column_names if col in df.columns] # Cria uma lista de colunas que existem tanto na lista de campos desejados quanto no DataFrame\n",
        "\n",
        "    df = df[existing_columns] # Seleciona apenas as colunas existentes no DataFrame\n",
        "    return df # Retorna o DataFrame\n",
        "\n",
        "# Função para criar um conjunto de dados no BigQuery caso ele não exista\n",
        "def create_dataset_if_not_exists(client, dataset_name):\n",
        "    \"\"\"\n",
        "    Cria um conjunto de dados no BigQuery se ele não existir.\n",
        "\n",
        "    Args:\n",
        "        client: O cliente do BigQuery.\n",
        "        dataset_name (str): O nome do conjunto de dados.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        client.get_dataset(dataset_name) # Tenta obter o conjunto de dados\n",
        "    except NotFound:\n",
        "        print(f\"Dataset {dataset_name} not found. Creating...\") # Imprime uma mensagem se o conjunto de dados não for encontrado\n",
        "        client.create_dataset(dataset_name) # Cria o conjunto de dados\n",
        "\n",
        "# Função para obter o esquema de uma tabela no BigQuery\n",
        "def get_table_schema(client, dataset_name, table_name):\n",
        "    \"\"\"\n",
        "    Obtém o esquema de uma tabela no BigQuery.\n",
        "\n",
        "    Args:\n",
        "        client: O cliente do BigQuery.\n",
        "        dataset_name (str): O nome do conjunto de dados.\n",
        "        table_name (str): O nome da tabela.\n",
        "\n",
        "    Returns:\n",
        "        google.cloud.bigquery.schema.SchemaField: O esquema da tabela.\n",
        "    \"\"\"\n",
        "    table_ref = client.dataset(dataset_name).table(table_name) # Cria uma referência à tabela\n",
        "    table = client.get_table(table_ref) # Obtém a tabela\n",
        "    return table.schema # Retorna o esquema da tabela\n",
        "\n",
        "# Função para carregar dados no BigQuery\n",
        "def load_data_to_bigquery(client, dataset_name, table_name, data, write_disposition, partition_column=None, clustering_fields=None):\n",
        "    \"\"\"\n",
        "    Carrega dados em uma tabela no BigQuery.\n",
        "\n",
        "    Args:\n",
        "        client: O cliente do BigQuery.\n",
        "        dataset_name (str): O nome do conjunto de dados.\n",
        "        table_name (str): O nome da tabela.\n",
        "        data (pd.DataFrame): O DataFrame contendo os dados a serem carregados.\n",
        "        write_disposition (str): A disposição de gravação para o trabalho de carregamento.\n",
        "        partition_column (str, optional): O nome da coluna de partição.\n",
        "        clustering_fields (list, optional): Uma lista de nomes de campos para agrupamento em cluster.\n",
        "    \"\"\"\n",
        "    table_ref = client.dataset(dataset_name).table(table_name) # Cria uma referência à tabela\n",
        "\n",
        "    try:\n",
        "        table = client.get_table(table_ref) # Tenta obter a tabela\n",
        "        schema = table.schema # Obtém o esquema da tabela\n",
        "    except NotFound:\n",
        "        schema = None # Define o esquema como None se a tabela não for encontrada\n",
        "        print(f\"Schema para a tabela {DATASET_NAME}.{table_name} não encontrada.\") # Imprime uma mensagem se o esquema não for encontrado\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        write_disposition=write_disposition, # Define a disposição de gravação\n",
        "    )\n",
        "    if schema:\n",
        "        job_config.schema = schema # Define o esquema se ele for encontrado\n",
        "    else:\n",
        "        job_config.autodetect = True # Habilita a detecção automática de esquema se o esquema não for encontrado\n",
        "\n",
        "    json_str = data.to_json(orient='records', date_format='iso') # Converte o DataFrame para JSON\n",
        "    job = client.load_table_from_json(json.loads(json_str), table_ref, job_config=job_config) # Cria um trabalho de carregamento para carregar os dados do JSON na tabela\n",
        "    job.result() # Aguarda a conclusão do trabalho de carregamento\n",
        "    print(f\"Foram carregadas {len(data)} linhas em {dataset_name}.{table_name}\") # Imprime uma mensagem com o número de linhas carregadas\n",
        "\n",
        "# Função para obter a última data de atualização da tabela de controle\n",
        "def get_last_update(client, now):\n",
        "    \"\"\"\n",
        "    Obtém a última data de atualização da tabela de controle.\n",
        "\n",
        "    Args:\n",
        "        client: O cliente do BigQuery.\n",
        "        now (datetime): A data e hora atuais.\n",
        "\n",
        "    Returns:\n",
        "        datetime: A última data de atualização.\n",
        "    \"\"\"\n",
        "    table_name = f'{PROJECT_ID}.{DATASET_NAME}.updates' # Define o nome da tabela de controle\n",
        "\n",
        "    try:\n",
        "        client.get_table(table_name) # Tenta obter a tabela de controle\n",
        "    except NotFound:\n",
        "        print(f\"Table {table_name} not found. Initializing with FULL_LOAD_DATE: {FULL_LOAD_DATE}\") # Imprime uma mensagem se a tabela de controle não for encontrada\n",
        "        full_load_date = datetime.strptime(FULL_LOAD_DATE, '%Y-%m-%d') # Converte a data de carregamento completo para datetime\n",
        "        initial_data = [{'updated_at': full_load_date}] # Cria uma lista de dicionários com a data de carregamento completo\n",
        "        load_data_to_bigquery(client, DATASET_NAME, 'updates', pd.DataFrame(initial_data), 'WRITE_TRUNCATE') # Carrega a data de carregamento completo na tabela de controle\n",
        "        return full_load_date # Retorna a data de carregamento completo\n",
        "\n",
        "    query = f'SELECT MAX(updated_at) AS last_update FROM `{table_name}`' # Define a consulta para obter a última data de atualização\n",
        "\n",
        "    try:\n",
        "        query_job = client.query(query) # Executa a consulta\n",
        "        results = query_job.result() # Obtém os resultados da consulta\n",
        "\n",
        "        for row in results: # Itera sobre os resultados da consulta\n",
        "            print(f\"Ultima data de atualização: {row.last_update}\") # Imprime a última data de atualização\n",
        "            return row.last_update if row.last_update else now - timedelta(days=1) # Retorna a última data de atualização ou a data atual menos 1 dia se a última data de atualização for None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while executing the query: {e}\") # Imprime uma mensagem se ocorrer um erro ao executar a consulta\n",
        "        return None # Retorna None se ocorrer um erro\n",
        "\n",
        "# Função para registrar a data e hora da atualização\n",
        "def log_update(client,now):\n",
        "    \"\"\"\n",
        "    Registra a data e hora da atualização na tabela de controle.\n",
        "\n",
        "    Args:\n",
        "        client: O cliente do BigQuery.\n",
        "        now (datetime): A data e hora atuais.\n",
        "    \"\"\"\n",
        "    table_name = f'{PROJECT_ID}.{DATASET_NAME}.updates' # Define o nome da tabela de controle\n",
        "    updated_at = [{'updated_at': now}] # Cria uma lista de dicionários com a data e hora atuais\n",
        "    load_data_to_bigquery(client, DATASET_NAME, 'updates', pd.DataFrame(updated_at), 'WRITE_APPEND') # Carrega a data e hora atuais na tabela de controle\n",
        "\n",
        "# Função para atualizar os parâmetros de data para a carga incremental\n",
        "def incremental_params_update(table, incremental_load_params, params, last_update, now):\n",
        "    \"\"\"\n",
        "    Atualiza os parâmetros de data para a carga incremental.\n",
        "\n",
        "    Args:\n",
        "        table (str): O nome da tabela.\n",
        "        incremental_load_params (dict): O dicionário de parâmetros de carga incremental.\n",
        "        params (dict): O dicionário de parâmetros da API.\n",
        "        last_update (datetime): A última data de atualização.\n",
        "        now (datetime): A data e hora atuais.\n",
        "\n",
        "    Returns:\n",
        "        dict: O dicionário de parâmetros da API atualizado.\n",
        "    \"\"\"\n",
        "    if table == \"past_fixtures\": # Verifica se a tabela é \"past_fixtures\"\n",
        "        params.update({ # Atualiza os parâmetros da API com as datas de início e fim da carga incremental\n",
        "            'from': '2022-01-01',  # Ajuste para o início do ano de 2022\n",
        "            'to': '2022-12-31'     # Ajuste para o final do ano de 2022\n",
        "        })\n",
        "    elif table == \"future_fixtures\": # Verifica se a tabela é \"future_fixtures\"\n",
        "        params.update({ # Atualiza os parâmetros da API com a data de início da carga incremental\n",
        "            'from': '2022-01-01',  # Ajuste para o início do ano de 2022\n",
        "            'to': '2022-12-31'     # Ajuste para o final do ano de 2022\n",
        "        })\n",
        "    else:\n",
        "        params # Retorna os parâmetros da API sem alterações se a tabela não for \"past_fixtures\" nem \"future_fixtures\"\n",
        "    return params # Retorna os parâmetros da API atualizados"
      ],
      "metadata": {
        "id": "JxgmxPp-TWWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função principal que orquestra a extração, transformação e carregamento dos dados\n",
        "def main(request=None):\n",
        "    \"\"\"\n",
        "    Função principal que orquestra a extração, transformação e carregamento dos dados.\n",
        "\n",
        "    Args:\n",
        "        request: A requisição HTTP.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Uma tupla contendo a mensagem de sucesso e o código de status HTTP.\n",
        "    \"\"\"\n",
        "    client = bigquery.Client(project=PROJECT_ID) # Cria um cliente do BigQuery\n",
        "    create_dataset_if_not_exists(client, DATASET_NAME) # Cria o conjunto de dados se ele não existir\n",
        "    now = datetime.now() # Obtém a data e hora atuais\n",
        "    # now = datetime(2022, 6, 1, 0, 0) # Para testes, defina uma data específica\n",
        "    last_update = get_last_update(client, now) # Obtém a última data de atualização\n",
        "    main_endpoints = endpoints.copy() # Faz uma cópia da lista de endpoints principais\n",
        "    main_iterable_endpoints = iterable_endpoints.copy() # Faz uma cópia do dicionário de endpoints iteráveis\n",
        "\n",
        "    for endpoint in main_endpoints: # Itera sobre cada endpoint principal\n",
        "        params = endpoint.get('params', {}) # Obtém os parâmetros do endpoint\n",
        "        table = endpoint.get('table') # Obtém o nome da tabela\n",
        "        incremental_load_params = endpoint.get('incremental_load_params') # Obtém os parâmetros de carga incremental\n",
        "        path = endpoint.get('path') # Obtém o caminho do endpoint\n",
        "        fields = endpoint.get('fields') # Obtém os campos a serem selecionados\n",
        "        nested_fields = endpoint.get('nested_fields') # Obtém os campos aninhados\n",
        "        repeatable_fields = endpoint.get('repeatable_fields') # Obtém os campos repetíveis\n",
        "        write_disposition = endpoint.get('write_disposition') # Obtém a disposição de gravação\n",
        "        quality_control = endpoint.get('quality_control', False) # Obtém a flag de controle de qualidade\n",
        "\n",
        "        if incremental_load_params: # Verifica se há parâmetros de carga incremental\n",
        "            params = incremental_params_update(table, incremental_load_params, params, last_update, now) # Atualiza os parâmetros da API com as datas de início e fim da carga incremental\n",
        "\n",
        "        raw_data = fetch_data(path, params, HEADERS) # Busca os dados brutos da API\n",
        "\n",
        "        if not raw_data:\n",
        "            print(f\"Endpoint {table} não retornou dados.\")  # Diagnóstico do porquê a tabela não está sendo criada\n",
        "        else:\n",
        "            print(f\"Dados obtidos para o endpoint {table}.\")\n",
        "\n",
        "        if raw_data: # Verifica se há dados brutos\n",
        "            prepared_data = prepare_dataframe(raw_data, fields, nested_fields, repeatable_fields) # Prepara o DataFrame\n",
        "            load_data_to_bigquery(client, DATASET_NAME, table, prepared_data, write_disposition) # Carrega os dados no BigQuery\n",
        "\n",
        "            if table in main_iterable_endpoints: # Verifica se há endpoints iteráveis para o endpoint principal atual\n",
        "                for iterable_endpoint in main_iterable_endpoints[table]: # Itera sobre cada endpoint iterável\n",
        "                    iterable_table = iterable_endpoint.get('table') # Obtém o nome da tabela do endpoint iterável\n",
        "                    iterable_fields = iterable_endpoint.get('fields') # Obtém os campos a serem selecionados do endpoint iterável\n",
        "                    iterable_nested_fields = iterable_endpoint.get('nested_fields') # Obtém os campos aninhados do endpoint iterável\n",
        "                    iterable_repeatable_fields = iterable_endpoint.get('repeatable_fields') # Obtém os campos repetíveis do endpoint iterável\n",
        "                    iterable_write_disposition = iterable_endpoint.get('write_disposition') # Obtém a disposição de gravação do endpoint iterável\n",
        "\n",
        "                    print(f\"Buscando os dados iteráveis para o endpoint: {iterable_table}\") # Imprime uma mensagem informando que está buscando dados iteráveis\n",
        "                    detailed_data = fetch_iterable_data(prepared_data, iterable_endpoint) # Busca os dados do endpoint iterável\n",
        "\n",
        "                    if detailed_data: # Verifica se há dados do endpoint iterável\n",
        "                        prepared_iterable_data = prepare_dataframe(detailed_data, iterable_fields, iterable_nested_fields, iterable_repeatable_fields) # Prepara o DataFrame do endpoint iterável\n",
        "                        load_data_to_bigquery(client, DATASET_NAME, iterable_table, prepared_iterable_data, iterable_write_disposition) # Carrega os dados do endpoint iterável no BigQuery\n",
        "                    else:\n",
        "                        print(f\"Não foram encontrados dados para o endpoint: {iterable_table}\") # Imprime uma mensagem se não houver dados do endpoint iterável\n",
        "        else:\n",
        "            print(f\"Não foram encontrados dados para o endpoint: {table}\") # Imprime uma mensagem se não houver dados brutos\n",
        "\n",
        "    log_update(client, now) # Registra a data e hora da atualização\n",
        "\n",
        "    return \"success\", 200 # Retorna uma mensagem de sucesso e o código de status HTTP 200\n",
        "\n",
        "main() # Executa a função principal se o script for executado como um programa principal"
